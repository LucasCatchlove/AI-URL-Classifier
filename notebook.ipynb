{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ofmPaJBG6bL"
      },
      "source": [
        "#**Casting the Net:  Machine Learning for Phishing Website Detection**\n",
        "\n",
        "##Abstract \n",
        "\n",
        "The goal of the project was to train a series of models to classify URLs as legimitimate or fraudulent (URLs that lead to phishing scams) and provide insights into the classification process. Decision Trees, Random Forests, and Neural networks were used as models while feature importances provided understanding as to how the model classifies samples. The Decision Tree and Random Forest models performed well with accuracies between 95% and 100%; The Neural Network implementation did not perform as well with accuracies maxing out at around 76%. Feature importance analysis revealed that certain features and feature categories played far more important roles in classifiying the URLs. Specifically, the structure of the filename and its associated directory in the URL as well as the information pertaining to the website, namely the hostname TTL, number of redirects, the time it took for the domain to be activated, etc.\n",
        "\n",
        "##Introduction\n",
        "\n",
        "Here you have to explain the problem that you are solving. Explain why it is important, and what are the main challenges. Mention previous attempts (add papers as references) to solve it. Mainly focus on the techniques closely related to our approach. Briefly describe your approach and explain why it is promising for solving the addressed problem. Mention the dataset and the main results achieved.\n",
        "\n",
        "Phishing is a common source of fraud on the internet that can have\n",
        "devastating effects on the lives of its victims. The goal of this project is to\n",
        "produce a model that can quickly identify whether or not a URL presented to a\n",
        "user is malicious as well as provide an intuitive understanding as to why the\n",
        "URL in question is malicious (or not) by extracting insights from the model by examining feature importance (only performed on decision tree and random forest models in this report).  The dataset used is composed of **88,647**\n",
        "training examples with **111 features** in total (the features are categorised into\n",
        "implicit groups that correspond to specific parts of a URLs anatomy such as\n",
        "the domain or protocol) [1]. \n",
        "\n",
        "This dataset is used to train a Decision Tree, Random Forest, and Neural Network to classify the URLs. Feature importance is then extracted from the Decision Tree and Random Forest models to provide insight into the importance of the datasets features in regards to classfication.\n",
        "\n",
        " The majority of existing attempts at phishing detection rely upon a mix of URL features as well as the content of the webpage(s) the URL redirects too, and often take a \"big data\" approach in regards to the dataset used; Specifically, the datasets are continuously supplemented via web scraping and similar techniques. One of the more interesting approaches supplemented the typical practice of URL feature extraction with a scoring system based on the URL search engine page rank as phishing websites often rank very low (versus the website it is trying to impersonate) [2]. Another interesting appraoch was taken by Google researchers were they used a combination of URL feature extraction, webpage content, whitelists and blacklists, and hosting details to inform their models [3].\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Experimental Setup**\n",
        "\n",
        "https://data.mendeley.com/datasets/72ptz43s9v/1\n",
        "\n",
        "Total number of instances: 88,647 \\\\\n",
        "Number of legitimate website instances (labeled as 0): 58,000 \\\\\n",
        "Number of phishing website instances (labeled as 1): 30,647 \\\\\n",
        "Total number of features: 111 \\\\\n",
        "\n",
        "train size = 62052 samples (70% of dataset) \\\\\n",
        "test size = 26595 samples (30% of the dataset)\n",
        "\n",
        "**why use a imbalanced dataset?**\n",
        "\n",
        "Due to the fact that most websites are legitimate rather than fraudulent, the imbalance reflects the reality of the problem space. While the code below can be configured to use a balanced or imbalanced dataset, the model performs better when the imbalanced dataset is used. \n",
        "\n",
        "**Models and Hyperparameters**\n",
        "\n",
        "Decision Tree implemented with Sklearn\n",
        "\n",
        "Random Forest implemented with Sklearn \n",
        "\n",
        "Neural Network (implemented with PyTorch) with the following parameters: \n",
        "\n",
        "* **learning rate**: 0.01\n",
        "\n",
        "* **weight decay**: 0.00005 \n",
        "\n",
        "* **momentum**: 0.87\n",
        "\n",
        "* **batch size**: 64"
      ],
      "metadata": {
        "id": "a8gJT7O1qkNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Methodology**\n",
        "\n",
        "1. process the data by separating the label column of the CSV file from the rest of the feature columns. \n",
        "\n",
        "2. Split the training data into training, validation, and test sets \n",
        "\n",
        "3. create Tensor copies of the aformentioned sets for use with the pytorch Neural Network. \n",
        "\n",
        "4. Train the Decision Tree and Random Forests and calculate their respective accuracies\n",
        "\n",
        "5. Extract feature importances based on Mean Decrease in Impurity (MDI) and feature permutation for both the Decision Tree and Random Forest\n",
        "\n",
        "6. Train Neural Network and calculate accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "hJ3uleB3pI4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Experimental Results**"
      ],
      "metadata": {
        "id": "AIccwx2Uqvyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XOkicNLmqDA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "RMtFJDI0Jg19"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import sklearn.ensemble\n",
        "import sklearn.inspection\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.inspection import permutation_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOMFCyk5d7n2"
      },
      "source": [
        "##Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('dataset_full.csv')\n",
        "\n",
        "#feature labels and number of features for later use\n",
        "feature_names = list(dataset.columns.values)[:111]\n",
        "num_features = len(feature_names)\n",
        "\n",
        "#split the csv into training data and labels\n",
        "X, y = dataset.iloc[:,0:111], dataset['phishing']\n",
        "\n",
        "#split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "#creation of validation sets for use with feature permutation from the training sets\n",
        "X_train, X_vali, y_train, y_vali = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "#Conversion to PyTorch Tensors for use with the Neural Network implementation \n",
        "X_train_torch = torch.Tensor(X_train.values)\n",
        "y_train_torch = torch.Tensor(y_train.values).long()\n",
        "\n",
        "X_test_torch = torch.Tensor(X_test.values)\n",
        "y_test_torch = torch.Tensor(y_test.values).long()"
      ],
      "metadata": {
        "id": "qxlrLXofHgl_"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PoE35_se_LW"
      },
      "source": [
        "#**Decision Tree Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgOnWav4fEvQ",
        "outputId": "3fbc19b4-2ccc-4cf7-ef66-7312e20bcfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.00% training accuracy\n",
            "0.00% training error\n",
            "95.21% test accuracy\n",
            "4.79% test error\n"
          ]
        }
      ],
      "source": [
        "#train the model\n",
        "tree = sklearn.tree.DecisionTreeClassifier(random_state=0).fit(X_train, y_train);\n",
        "\n",
        "#training predictions and accuracy\n",
        "y_pred = tree.predict(X_train)\n",
        "\n",
        "training_accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n",
        "\n",
        "#test predictions and accuracy\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('{:.2%} training accuracy'.format(training_accuracy))\n",
        "print('{:.2%} training error'.format(1-training_accuracy))\n",
        "print('{:.2%} test accuracy'.format(test_accuracy))\n",
        "print('{:.2%} test error'.format(1-test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Importances"
      ],
      "metadata": {
        "id": "cSohYpeF2GyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mean Decrease In Impurity (MDI)"
      ],
      "metadata": {
        "id": "15XK5UNKTlyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "gdu5CE1ffnkf"
      },
      "outputs": [],
      "source": [
        "#extract the feature importances (MDI values) from the Decision Tree model object\n",
        "importances = tree.feature_importances_\n",
        "tree_importances = pd.Series(importances, index=feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the values with associated feature names\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20,40)\n",
        "tree_importances.plot.barh(ax=ax, align='center')\n",
        "plt.title(\"Feature importances using 'Mean Decrease in Impurity (MDI)\")\n",
        "plt.ylabel(\"Mean decrease in impurity\");"
      ],
      "metadata": {
        "id": "Y9yKoTuoPagT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Permutation\n",
        "\n"
      ],
      "metadata": {
        "id": "41a0IAmxJYb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature importance mean calculation based on 10 runs (n_repeats)\n",
        "result = permutation_importance(tree, X_vali, y_vali, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "#mean of the feature importances over the 10 runs\n",
        "importances = result.importances_mean"
      ],
      "metadata": {
        "id": "UgKioYprJeCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting of the feature importances with associated labels\n",
        "plt.figure(figsize=(20,40))\n",
        "plt.barh(range(X.shape[1]), importances, align='center')\n",
        "plt.yticks(range(X.shape[1]), feature_names)\n",
        "plt.title('Feature Importance using Feature Permutation')\n",
        "plt.xlabel('Feature Importance ')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_roiEgbnKVle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyg98Ol4dxfO"
      },
      "source": [
        "#**Random Forest Implementation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgQsUzl15Szr",
        "outputId": "0aeaddf5-56d3-4939-c6ff-17c7ef9aea67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.00% training accuracy\n",
            "0.00% training error\n",
            "96.80% test accuracy\n",
            "3.20% test error\n"
          ]
        }
      ],
      "source": [
        "#train the model\n",
        "forest = sklearn.ensemble.RandomForestClassifier(random_state=0).fit(X_train, y_train);\n",
        "\n",
        "#training predictions and accuracy\n",
        "y_pred = forest.predict(X_train)\n",
        "\n",
        "training_accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n",
        "\n",
        "#test predictions and accuracy\n",
        "y_pred = forest.predict(X_test)\n",
        "\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('{:.2%} training accuracy'.format(training_accuracy))\n",
        "print('{:.2%} training error'.format(1-training_accuracy))\n",
        "print('{:.2%} test accuracy'.format(test_accuracy))\n",
        "print('{:.2%} test error'.format(1-test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Importances"
      ],
      "metadata": {
        "id": "oMsUGfUX3pzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mean Decrease in Impurity (MDI)\n"
      ],
      "metadata": {
        "id": "Rxawr5pLT1Be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "BzDYlZrA5qXV"
      },
      "outputs": [],
      "source": [
        "#extract the feature importances (MDI values) from the Random Forest model object\n",
        "importances = forest.feature_importances_\n",
        "forest_importances = pd.Series(importances, index=feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the values with associated feature names\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20,40)\n",
        "forest_importances.plot.barh(ax=ax, align='center')\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")"
      ],
      "metadata": {
        "id": "Wi8AQX3rPOqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Permutation"
      ],
      "metadata": {
        "id": "9lt_AJLcM4BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature importance mean calculation based on 10 runs (n_repeats)\n",
        "result = permutation_importance(forest, X_vali, y_vali, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "#mean of the feature importances over the 10 runs\n",
        "importances = result.importances_mean"
      ],
      "metadata": {
        "id": "BOxN0vwNM7qK"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the values with associated feature names\n",
        "plt.figure(figsize=(20,40))\n",
        "plt.barh(range(X.shape[1]), importances, align='center')\n",
        "plt.yticks(range(X.shape[1]), feature_names)\n",
        "plt.title('Feature Importance using Feature Permutation')\n",
        "plt.xlabel('Feature Importance ')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BvgGtGpNM8qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c9SbtnIdghI"
      },
      "source": [
        "##**Neural Network Implementation**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create batches and shuffle\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#network architecture\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(111, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 384),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(384, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 192),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(192, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 96),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(96, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "#optimizer and loss function with momentum and regularization (weight_decay)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.00005, momentum=0.87)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "#training loop with mini-batching\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    \n",
        "    #minibatching\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        labels = labels.view(-1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels)\n",
        "    \n",
        "    #epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0frfYqufXTIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**References**\n",
        "\n",
        "**[1]** Vrbančič, Grega (2020), “Phishing Websites Dataset”, Mendeley Data, V1, doi: 10.17632/72ptz43s9v.1\n",
        "\n",
        "**[2]** \"A novel Phishing classification based on URL features.\" IEEE Xplore. https://ieeexplore.ieee.org/abstract/document/5752505 (accessed Apr. 24, 2023).\n",
        "\n",
        "**[3]** \"Large-Scale Automatic Classification of Phishing Pages â Google Research.\" Google Research. https://research.google/pubs/pub35580/ (accessed Apr. 24, 2023).\n",
        "\n"
      ],
      "metadata": {
        "id": "N4CYM-a1Ue1s"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}